{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSql学习"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: master\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 11:16:29 WARN Utils: Your hostname, ubuntu-linux-22-04-desktop resolves to a loopback address: 127.0.1.1; using 10.211.55.3 instead (on interface enp0s5)\n",
      "23/03/31 11:16:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/31 11:16:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "#指定spark_home为刚才的解压路径,指定python路径\n",
    "spark_home = \"/home/parallels/Desktop/work/spark-3.3.2-bin-hadoop3\"\n",
    "python_path = \"/home/parallels/anaconda3/bin/python3\"\n",
    "findspark.init(spark_home,python_path)\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#SparkSQL的许多功能封装在SparkSession的方法接口中\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"test\") \\\n",
    "        .config(\"master\",\"local[4]\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、RDD，DataFrame和DataSet对比\n",
    "DataFrame参照了Pandas的思想，在RDD基础上增加了schma，能够获取列名信息。\n",
    "\n",
    "DataSet在DataFrame基础上进一步增加了数据类型信息，可以在编译时发现类型错误。\n",
    "\n",
    "DataFrame可以看成DataSet[Row]，两者的API接口完全相同。\n",
    "\n",
    "DataFrame和DataSet都支持SQL交互式查询，可以和 Hive无缝衔接。\n",
    "\n",
    "DataSet只有Scala语言和Java语言接口中才支持，在Python和R语言接口只支持DataFrame。\n",
    "\n",
    "DataFrame数据结构本质上是通过RDD来实现的，但是RDD是一种行存储的数据结构，而DataFrame是一种列存储的数据结构。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、创建DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 通过toDF方法转换成DataFrame\n",
    "可以将RDD用toDF方法转换成DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|     name|age|score|\n",
      "+---------+---+-----+\n",
      "|    LiLei| 15|   88|\n",
      "|HanMeiMei| 16|   90|\n",
      "|   DaChui| 17|   60|\n",
      "+---------+---+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将RDD转换成DataFrame\n",
    "rdd = sc.parallelize([(\"LiLei\",15,88),(\"HanMeiMei\",16,90),(\"DaChui\",17,60)])\n",
    "df = rdd.toDF([\"name\",\"age\",\"score\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 通过createDataFrame方法将Pandas.DataFrame转换成pyspark中的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/parallels/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    LiLei| 18|\n",
      "|HanMeiMei| 17|\n",
      "+---------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([(\"LiLei\",18),(\"HanMeiMei\",17)],columns = [\"name\",\"age\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    LiLei| 18|\n",
      "|HanMeiMei| 17|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 也可以对列表直接转换\n",
    "values = [(\"LiLei\",18),(\"HanMeiMei\",17)]\n",
    "df = spark.createDataFrame(values,[\"name\",\"age\"])\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 通过createDataFrame方法指定schema动态创建DataFrame\n",
    "可以通过createDataFrame的方法指定rdd和schema创建DataFrame。\n",
    "\n",
    "这种方法比较繁琐，但是可以在预先不知道schema和数据类型的情况下在代码中动态创建DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+\n",
      "|     name|score|  birthday|\n",
      "+---------+-----+----------+\n",
      "|    LiLei|   87|2010-01-05|\n",
      "|HanMeiMei|   90|2009-03-01|\n",
      "|   DaChui| null|2008-07-02|\n",
      "+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([StructField(\"name\", StringType(), nullable = False),\n",
    "                     StructField(\"score\", IntegerType(), nullable = True),\n",
    "                     StructField(\"birthday\", DateType(), nullable = True)])\n",
    "\n",
    "rdd = sc.parallelize([Row(\"LiLei\",87,datetime(2010,1,5)),\n",
    "                      Row(\"HanMeiMei\",90,datetime(2009,3,1)),\n",
    "                      Row(\"DaChui\",None,datetime(2008,7,2))])\n",
    "\n",
    "dfstudent = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "dfstudent.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 通过读取文件创建\n",
    "可以读取json文件，csv文件，hive数据表或者mysql数据表得到DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取json文件生成DataFrame\n",
    "df = spark.read.json(\"../data/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2|    0|\n",
      "|        4.9|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|        1.3|       0.2|    0|\n",
      "|        4.6|       3.1|        1.5|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取csv文件\n",
    "df = spark.read.option(\"header\",\"true\") \\\n",
    " .option(\"inferSchema\",\"true\") \\\n",
    " .option(\"delimiter\", \",\") \\\n",
    " .csv(\"../data/iris.csv\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2|    0|\n",
      "|        4.9|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|        1.3|       0.2|    0|\n",
      "|        4.6|       3.1|        1.5|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取csv文件\n",
    "df = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    " .option(\"header\",\"true\") \\\n",
    " .option(\"inferSchema\",\"true\") \\\n",
    " .option(\"delimiter\", \",\") \\\n",
    " .load(\"../data/iris.csv\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取parquet文件\n",
    "df = spark.read.parquet(\"../data/users.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  0|val_0|\n",
      "|  0|val_0|\n",
      "|  0|val_0|\n",
      "|  2|val_2|\n",
      "|  4|val_4|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#读取hive数据表生成DataFrame\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH '../data/kv1.txt' INTO TABLE src\")\n",
    "df = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取mysql数据表生成DataFrame\n",
    "\"\"\"\n",
    "url = \"jdbc:mysql://localhost:3306/test\"\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    " .option(\"url\", url) \\\n",
    " .option(\"dbtable\", \"runoob_tbl\") \\\n",
    " .option(\"user\", \"root\") \\\n",
    " .option(\"password\", \"0845\") \\\n",
    " .load()\\\n",
    "df.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、DataFrame保存成文件\n",
    "可以保存成csv文件，json文件，parquet文件或者保存成hive数据表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存成csv文件\n",
    "df = spark.read.format(\"json\").load(\"../data/people.json\")\n",
    "df.write.format(\"csv\").option(\"header\",\"true\").save(\"data/people_write.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先转换成rdd再保存成txt文件\n",
    "df.rdd.saveAsTextFile(\"data/people_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存成json文件\n",
    "df.write.json(\"data/people_write.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存成parquet文件, 压缩格式, 占用存储小, 且是spark内存中存储格式，加载最快\n",
    "df.write.partitionBy(\"age\").format(\"parquet\").save(\"data/namesAndAges.parquet\")\n",
    "df.write.parquet(\"data/people_write.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存成hive数据表\n",
    "df.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame的API交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     name|age|gender|\n",
      "+---------+---+------+\n",
      "|    LiLei| 15|  male|\n",
      "|HanMeiMei| 16|female|\n",
      "|   DaChui| 17|  male|\n",
      "+---------+---+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(\"LiLei\",15,\"male\"),\n",
    "     (\"HanMeiMei\",16,\"female\"),\n",
    "     (\"DaChui\",17,\"male\")]).toDF(\"name\",\"age\",\"gender\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Action操作\n",
    "DataFrame的Action操作包括show,count,collect,,describe,take,head,first等操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     name|age|gender|\n",
      "+---------+---+------+\n",
      "|    LiLei| 15|  male|\n",
      "|HanMeiMei| 16|female|\n",
      "|   DaChui| 17|  male|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|name     |age|gender|\n",
      "+---------+---+------+\n",
      "|LiLei    |15 |male  |\n",
      "|HanMeiMei|16 |female|\n",
      "+---------+---+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show(numRows: Int, truncate: Boolean) \n",
    "#第二个参数设置是否当输出字段长度超过20时进行截取\n",
    "df.show(2,False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='LiLei', age=15, gender='male'),\n",
       " Row(name='HanMeiMei', age=16, gender='female'),\n",
       " Row(name='DaChui', age=17, gender='male')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='LiLei', age=15, gender='male')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='LiLei', age=15, gender='male'),\n",
       " Row(name='HanMeiMei', age=16, gender='female')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#head\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='LiLei', age=15, gender='male'),\n",
       " Row(name='HanMeiMei', age=16, gender='female')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#head\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 类RDD操作\n",
    "DataFrame支持RDD中一些诸如distinct,cache,sample,foreach,intersect,except等操作。\n",
    "\n",
    "可以把DataFrame当做数据类型为Row的RDD来进行操作，必要时可以将其转换成RDD来操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|Hello World|\n",
      "|Hello China|\n",
      "|Hello Spark|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Hello World\",),(\"Hello China\",),(\"Hello Spark\",)]).toDF(\"value\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46a25d2ee2cf42d8b5967fa87ecb381310ee627e58dce25c5456af41f27dd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
