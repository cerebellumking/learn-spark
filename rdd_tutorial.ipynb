{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "#指定spark_home为刚才的解压路径,指定python路径\n",
    "spark_home = \"/home/parallels/Desktop/work/spark-3.3.2-bin-hadoop3\"\n",
    "python_path = \"/home/parallels/anaconda3/bin/python3\"\n",
    "findspark.init(spark_home,python_path)\n",
    "\n",
    "conf = SparkConf().setAppName(\"rdd_tutorial\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world',\n",
       " 'hello spark',\n",
       " 'spark love jupyter',\n",
       " 'spark love pandas',\n",
       " 'spark love sql']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从本地文件系统中加载数据\n",
    "file = \"../data/hello.txt\"\n",
    "rdd = sc.textFile(file,3)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从集群文件系统中加载数据\n",
    "#file = \"hdfs://localhost:9000/user/hadoop/data.txt\"\n",
    "#也可以省去hdfs://localhost:9000\n",
    "#rdd = sc.textFile(file,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "#parallelize将Driver中的数据结构生成RDD,第二个参数指定分区数\n",
    "rdd = sc.parallelize(range(1,11),2)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、常用Action操作\n",
    "Action操作将触发基于RDD依赖关系的计算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10),5) \n",
    "#collect操作将数据汇集到Driver,数据过大时有超内存风险\n",
    "all_data = rdd.collect()\n",
    "all_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take操作将前若干个数据汇集到Driver，相比collect安全\n",
    "rdd = sc.parallelize(range(10),5) \n",
    "part_data = rdd.take(4)\n",
    "part_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 1, 5, 3, 4, 2, 0, 9, 6]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeSample可以随机取若干个到Driver,第一个参数设置是否放回抽样\n",
    "rdd = sc.parallelize(range(10),5) \n",
    "sample_data = rdd.takeSample(False,10,0)\n",
    "sample_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#first取第一个数据\n",
    "rdd = sc.parallelize(range(10),5) \n",
    "first_data = rdd.first()\n",
    "print(first_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#count查看RDD元素数量\n",
    "rdd = sc.parallelize(range(10),5)\n",
    "data_count = rdd.count()\n",
    "print(data_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce利用二元函数对数据进行规约\n",
    "rdd = sc.parallelize(range(10),5) \n",
    "rdd.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "#foreach对每一个元素执行某种操作，不生成新的RDD\n",
    "#累加器用法详见共享变量\n",
    "rdd = sc.parallelize(range(10),5) \n",
    "accum = sc.accumulator(0)\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 3: 1, 2: 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByKey对Pair RDD按key统计数量\n",
    "pairRdd = sc.parallelize([(1,1),(1,4),(3,9),(2,16)]) \n",
    "pairRdd.countByKey()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveAsTextFile保存rdd成text文件到本地\n",
    "text_file = \"./data/rdd.txt\"\n",
    "rdd = sc.parallelize(range(5))\n",
    "rdd.saveAsTextFile(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '3', '4', '2', '0']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重新读入会被解析文本\n",
    "rdd_loaded = sc.textFile(text_file)\n",
    "rdd_loaded.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、常用Transformation操作\n",
    "Transformation转换操作具有懒惰执行的特性，它只指定新的RDD和其父RDD的依赖关系，只有当Action操作触发到该依赖的时候，它才被计算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map操作对每个元素进行一个映射转换\n",
    "rdd = sc.parallelize(range(10),3)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x:x**2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter应用过滤条件过滤掉一些数据\n",
    "rdd = sc.parallelize(range(10),3)\n",
    "rdd.filter(lambda x:x>5).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['hello', 'China']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap操作执行将每个元素生成一个Array后压平\n",
    "rdd = sc.parallelize([\"hello world\",\"hello China\"])\n",
    "rdd.map(lambda x:x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'hello', 'China']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x:x.split(\" \")).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample对原rdd在每个分区按照比例进行抽样,第一个参数设置是否可以重复抽样\n",
    "rdd = sc.parallelize(range(10),1)\n",
    "rdd.sample(False,0.5,0).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 5, 2, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distinct去重\n",
    "rdd = sc.parallelize([1,1,2,2,3,3,4,5])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### substract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtract找到属于前一个rdd而不属于后一个rdd的元素\n",
    "a = sc.parallelize(range(10))\n",
    "b = sc.parallelize(range(5,15))\n",
    "a.subtract(b).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#union合并数据\n",
    "a = sc.parallelize(range(5))\n",
    "b = sc.parallelize(range(3,8))\n",
    "a.union(b).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersection求交集\n",
    "a = sc.parallelize(range(1,6))\n",
    "b = sc.parallelize(range(3,9))\n",
    "a.intersection(b).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LiLei', 'HanMeiMei'),\n",
       " ('LiLei', 'Lily'),\n",
       " ('Tom', 'HanMeiMei'),\n",
       " ('Tom', 'Lily')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cartesian笛卡尔积\n",
    "boys = sc.parallelize([\"LiLei\",\"Tom\"])\n",
    "girls = sc.parallelize([\"HanMeiMei\",\"Lily\"])\n",
    "boys.cartesian(girls).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1, 1), (3, 2, 2), (1, 2, 3)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按照某种方式进行排序\n",
    "#指定按照第3个元素大小进行排序\n",
    "rdd = sc.parallelize([(1,2,3),(3,2,2),(4,1,1)])\n",
    "rdd.sortBy(lambda x:x[2]).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LiLei', 19), ('Hanmeimei', 18), ('Lily', 20)]\n"
     ]
    }
   ],
   "source": [
    "#按照拉链方式连接两个RDD，效果类似python的zip函数\n",
    "#需要两个RDD具有相同的分区，每个分区元素数量相同\n",
    "\n",
    "rdd_name = sc.parallelize([\"LiLei\",\"Hanmeimei\",\"Lily\"])\n",
    "rdd_age = sc.parallelize([19,18,20])\n",
    "\n",
    "rdd_zip = rdd_name.zip(rdd_age)\n",
    "print(rdd_zip.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LiLei', 0), ('Hanmeimei', 1), ('Lily', 2), ('Lucy', 3), ('Ann', 4), ('Dachui', 5), ('RuHua', 6)]\n"
     ]
    }
   ],
   "source": [
    "#将RDD和一个从0开始的递增序列按照拉链方式连接。\n",
    "rdd_name =  sc.parallelize([\"LiLei\",\"Hanmeimei\",\"Lily\",\"Lucy\",\"Ann\",\"Dachui\",\"RuHua\"])\n",
    "rdd_index = rdd_name.zipWithIndex()\n",
    "print(rdd_index.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、常用PairRDD的转换操作\n",
    "PairRDD指的是数据为长度为2的tuple类似(k,v)结构的数据类型的RDD,其每个数据的第一个元素被当做key，第二个元素被当做value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 4), ('world', 7)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduceByKey对相同的key对应的values应用二元归并操作\n",
    "rdd = sc.parallelize([(\"hello\",1),(\"world\",2),(\"hello\",3),(\"world\",5)])\n",
    "rdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', <pyspark.resultiterable.ResultIterable at 0xffff6a110880>),\n",
       " ('world', <pyspark.resultiterable.ResultIterable at 0xffff6a1128c0>)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupByKey将相同的key对应的values收集成一个Iterator\n",
    "rdd = sc.parallelize([(\"hello\",1),(\"world\",2),(\"hello\",3),(\"world\",5)])\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sortByKey按照key排序,可以指定是否降序\n",
    "rdd = sc.parallelize([(\"hello\",1),(\"world\",2),(\"China\",3),(\"Beijing\",5)])\n",
    "rdd.sortByKey(False).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join相当于根据key进行内连接\n",
    "age = sc.parallelize([(\"LiLei\",18),(\"HanMeiMei\",16),(\"Jim\",20)])\n",
    "gender = sc.parallelize([(\"LiLei\",\"male\"),(\"HanMeiMei\",\"female\"),(\"Lucy\",\"female\")])\n",
    "age.join(gender).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### leftOuterJoin和rightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leftOuterJoin相当于关系表的左连接\n",
    "age = sc.parallelize([(\"LiLei\",18),(\"HanMeiMei\",16)])\n",
    "gender = sc.parallelize([(\"LiLei\",\"male\"),(\"HanMeiMei\",\"female\"),(\"Lucy\",\"female\")])\n",
    "age.leftOuterJoin(gender).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rightOuterJoin相当于关系表的右连接\n",
    "age = sc.parallelize([(\"LiLei\",18),(\"HanMeiMei\",16),(\"Jim\",20)])\n",
    "gender = sc.parallelize([(\"LiLei\",\"male\"),(\"HanMeiMei\",\"female\")])\n",
    "age.rightOuterJoin(gender).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cogroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (<pyspark.resultiterable.ResultIterable object at 0xffff6a1b0fa0>, <pyspark.resultiterable.ResultIterable object at 0xffff6a110610>)), ('b', (<pyspark.resultiterable.ResultIterable object at 0xffff6a112560>, <pyspark.resultiterable.ResultIterable object at 0xffff6a113b20>))]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "#cogroup相当于对两个输入分别goupByKey然后再对结果进行groupByKey\n",
    "\n",
    "x = sc.parallelize([(\"a\",1),(\"b\",2),(\"a\",3)])\n",
    "y = sc.parallelize([(\"a\",2),(\"b\",3),(\"b\",5)])\n",
    "\n",
    "result = x.cogroup(y).collect()\n",
    "print(result)\n",
    "print(list(result[0][1][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtractByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 3)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtractByKey去除x中那些key也在y中的元素\n",
    "\n",
    "x = sc.parallelize([(\"a\",1),(\"b\",2),(\"c\",3)])\n",
    "y = sc.parallelize([(\"a\",2),(\"b\",(1,2))])\n",
    "\n",
    "x.subtractByKey(y).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foldByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 10)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#foldByKey的操作和reduceByKey类似，但是要提供一个初始值\n",
    "x = sc.parallelize([(\"a\",1),(\"b\",2),(\"a\",3),(\"b\",5)],1)\n",
    "x.foldByKey(1,lambda x,y:x*y).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、缓存操作\n",
    "如果一个rdd被多个任务用作中间量，那么对其进行cache缓存到内存中对加快计算会非常有帮助。\n",
    "\n",
    "声明对一个rdd进行cache后，该rdd不会被立即缓存，而是等到它第一次被计算出来时才进行缓存。\n",
    "\n",
    "可以使用persist明确指定存储级别，常用的存储级别是MEMORY_ONLY和EMORY_AND_DISK。\n",
    "\n",
    "如果一个RDD后面不再用到，可以用unpersist释放缓存，unpersist是立即执行的。\n",
    "\n",
    "缓存数据不会切断血缘依赖关系，这是因为缓存数据某些分区所在的节点有可能会有故障，例如内存溢出或者节点损坏。\n",
    "\n",
    "这时候可以根据血缘关系重新计算这个分区的数据。\n",
    "\n",
    "如果要切断血缘关系，可以用checkpoint设置检查点将某个rdd保存到磁盘中。\n",
    "\n",
    "声明对一个rdd进行checkpoint后，该rdd不会被立即保存到磁盘，而是等到它第一次被计算出来时才保存成检查点。\n",
    "\n",
    "通常只对一些计算代价非常高昂的中间结果或者重复计算结果不可保证完全一致的情形下(如zipWithIndex算子)使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999.5\n"
     ]
    }
   ],
   "source": [
    "#cache缓存到内存中，使用存储级别 MEMORY_ONLY。\n",
    "#MEMORY_ONLY意味着如果内存存储不下，放弃存储其余部分，需要时重新计算。\n",
    "a = sc.parallelize(range(10000),5)\n",
    "a.cache()\n",
    "sum_a = a.reduce(lambda x,y:x+y)\n",
    "cnt_a = a.count()\n",
    "mean_a = sum_a/cnt_a\n",
    "\n",
    "print(mean_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999.5\n"
     ]
    }
   ],
   "source": [
    "#persist缓存到内存或磁盘中，默认使用存储级别MEMORY_AND_DISK\n",
    "#MEMORY_AND_DISK意味着如果内存存储不下，其余部分存储到磁盘中。\n",
    "#persist可以指定其它存储级别，cache相当于persist(MEMORY_ONLY)\n",
    "from  pyspark.storagelevel import StorageLevel\n",
    "a = sc.parallelize(range(10000),5)\n",
    "a.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "sum_a = a.reduce(lambda x,y:x+y)\n",
    "cnt_a = a.count()\n",
    "mean_a = sum_a/cnt_a\n",
    "\n",
    "a.unpersist() #立即释放缓存\n",
    "print(mean_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LiLei', 0), ('Hanmeimei', 1), ('LiLy', 2)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint 将数据设置成检查点，写入到磁盘中。\n",
    "sc.setCheckpointDir(\"./data/checkpoint/\")\n",
    "rdd_students = sc.parallelize([\"LiLei\",\"Hanmeimei\",\"LiLy\",\"Ann\"],2)\n",
    "\n",
    "rdd_students_idx = rdd_students.zipWithIndex() \n",
    "\n",
    "#设置检查点后，可以避免重复计算，不会因为zipWithIndex重复计算触发不一致的问题\n",
    "rdd_students_idx.checkpoint() \n",
    "rdd_students_idx.take(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、共享变量\n",
    "当spark集群在许多节点上运行一个函数时，默认情况下会把这个函数涉及到的对象在每个节点生成一个副本。\n",
    "\n",
    "但是，有时候需要在不同节点或者节点和Driver之间共享变量。\n",
    "\n",
    "Spark提供两种类型的共享变量，广播变量和累加器。\n",
    "\n",
    "广播变量是不可变变量，实现在不同节点不同任务之间共享数据。\n",
    "\n",
    "广播变量在每个机器上缓存一个只读的变量，而不是为每个task生成一个副本，可以减少数据的传输。\n",
    "\n",
    "累加器主要是不同节点和Driver之间共享变量，只能实现计数或者累加功能。\n",
    "\n",
    "累加器的值只有在Driver上是可读的，在节点上不可见。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 101, 102, 103, 104, 105, 106, 107, 108, 109]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#广播变量 broadcast 不可变，在所有节点可读\n",
    "\n",
    "broads = sc.broadcast(100)\n",
    "\n",
    "rdd = sc.parallelize(range(10))\n",
    "print(rdd.map(lambda x:x+broads.value).collect())\n",
    "\n",
    "print(broads.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#累加器 只能在Driver上可读，在其它节点只能进行累加\n",
    "\n",
    "total = sc.accumulator(0)\n",
    "rdd = sc.parallelize(range(10),3)\n",
    "\n",
    "rdd.foreach(lambda x:total.add(x))\n",
    "total.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5999999999999996"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算数据的平均值\n",
    "rdd = sc.parallelize([1.1,2.1,3.1,4.1])\n",
    "total = sc.accumulator(0)\n",
    "count = sc.accumulator(0)\n",
    "\n",
    "def func(x):\n",
    "    total.add(x)\n",
    "    count.add(1)\n",
    "    \n",
    "rdd.foreach(func)\n",
    "\n",
    "total.value/count.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、分区操作\n",
    "分区操作包括改变分区操作，以及针对分区执行的一些转换操作。\n",
    "\n",
    "glom：将一个分区内的数据转换为一个列表作为一行。\n",
    "\n",
    "coalesce：shuffle可选，默认为False情况下窄依赖，不能增加分区。repartition和partitionBy调用它实现。\n",
    "\n",
    "repartition：按随机数进行shuffle，相同key不一定在同一个分区\n",
    "\n",
    "partitionBy：按key进行shuffle，相同key放入同一个分区\n",
    "\n",
    "HashPartitioner：默认分区器，根据key的hash值进行分区，相同的key进入同一分区，效率较高，key不可为Array.\n",
    "\n",
    "RangePartitioner：只在排序相关函数中使用，除相同的key进入同一分区，相邻的key也会进入同一分区，key必须可排序。\n",
    "\n",
    "TaskContext: 获取当前分区id方法 TaskContext.get.partitionId\n",
    "\n",
    "mapPartitions：每次处理分区内的一批数据，适合需要分批处理数据的情况，比如将数据插入某个表，每批数据只需要开启一次数据库连接，大大减少了连接开支\n",
    "\n",
    "mapPartitionsWithIndex：类似mapPartitions，提供了分区索引，输入参数为（i，Iterator）\n",
    "\n",
    "foreachPartition：类似foreach，但每次提供一个Partition的一批数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#glom将一个分区内的数据转换为一个列表作为一行。\n",
    "a = sc.parallelize(range(10),2)\n",
    "b = a.glom()\n",
    "b.collect() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "#coalesce 默认shuffle为False，不能增加分区，只能减少分区\n",
    "#如果要增加分区，要设置shuffle = true\n",
    "#parallelize等许多操作可以指定分区数\n",
    "a = sc.parallelize(range(10),3)  \n",
    "print(a.getNumPartitions())\n",
    "print(a.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "b = a.coalesce(2) \n",
    "print(b.glom().collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 8, 9], [3, 4, 5], [], [0, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "#repartition按随机数进行shuffle，相同key不一定在一个分区，可以增加分区\n",
    "#repartition实际上调用coalesce实现，设置了shuffle = True\n",
    "a = sc.parallelize(range(10),3)  \n",
    "c = a.repartition(4) \n",
    "print(c.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 1), ('a', 2), ('c', 3)], [('a', 1)]]\n"
     ]
    }
   ],
   "source": [
    "#repartition按随机数进行shuffle，相同key不一定在一个分区\n",
    "a = sc.parallelize([(\"a\",1),(\"a\",1),(\"a\",2),(\"c\",3)])  \n",
    "c = a.repartition(2)\n",
    "print(c.glom().collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('c', 3)], [('a', 1), ('a', 1), ('a', 2)]]\n"
     ]
    }
   ],
   "source": [
    "#partitionBy按key进行shuffle，相同key一定在一个分区\n",
    "a = sc.parallelize([(\"a\",1),(\"a\",1),(\"a\",2),(\"c\",3)])  \n",
    "c = a.partitionBy(2)\n",
    "print(c.glom().collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitions可以对每个分区分别执行操作\n",
    "#每次处理分区内的一批数据，适合需要按批处理数据的情况\n",
    "#例如将数据写入数据库时，可以极大的减少连接次数。\n",
    "#mapPartitions的输入分区内数据组成的Iterator，其输出也需要是一个Iterator\n",
    "#以下例子查看每个分区内的数据,相当于用mapPartitions实现了glom的功能。\n",
    "a = sc.parallelize(range(10),2)\n",
    "a.mapPartitions(lambda it:iter([list(it)])).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0|10', '1|45']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapPartitionsWithIndex可以获取两个参数\n",
    "#即分区id和每个分区内的数据组成的Iterator\n",
    "a = sc.parallelize(range(11),2)\n",
    "\n",
    "def func(pid,it):\n",
    "    s = sum(it)\n",
    "    return(iter([str(pid) + \"|\" + str(s)]))\n",
    "    [str(pid) + \"|\" + str]\n",
    "b = a.mapPartitionsWithIndex(func)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (1, 2), (2, 3), (2, 4)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用TaskContext可以获取当前每个元素的分区\n",
    "from pyspark.taskcontext import TaskContext\n",
    "a = sc.parallelize(range(5),3)\n",
    "c = a.map(lambda x:(TaskContext.get().partitionId(),x))\n",
    "c.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [(0, 0), (1, 1), (1, 2), (2, 3), (2, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#foreachPartition对每个分区分别执行操作\n",
    "#范例：求每个分区内最大值的和\n",
    "total = sc.accumulator(0.0)\n",
    "\n",
    "a = sc.parallelize(range(1,101),3)\n",
    "\n",
    "def func(it):\n",
    "    total.add(max(it))\n",
    "    \n",
    "a.foreachPartition(func)\n",
    "total.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 20)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregate是一个Action操作\n",
    "#aggregate比较复杂，先对每个分区执行一个函数，再对每个分区结果执行一个合并函数。\n",
    "#例子：求元素之和以及元素个数\n",
    "#三个参数，第一个参数为初始值，第二个为分区执行函数，第三个为结果合并执行函数。\n",
    "rdd = sc.parallelize(range(1,21),3)\n",
    "def inner_func(t,x):\n",
    "    return((t[0]+x,t[1]+1))\n",
    "\n",
    "def outer_func(p,q):\n",
    "    return((p[0]+q[0],p[1]+q[1]))\n",
    "\n",
    "rdd.aggregate((0,0),inner_func,outer_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 3), ('a', 2), ('c', 2)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregateByKey的操作和aggregate类似，但是会对每个key分别进行操作\n",
    "#第一个参数为初始值，第二个参数为分区内归并函数，第三个参数为分区间归并函数\n",
    "\n",
    "a = sc.parallelize([(\"a\",1),(\"b\",1),(\"c\",2),(\"a\",2),(\"b\",3)],3)\n",
    "b = a.aggregateByKey(0,lambda x,y:max(x,y),lambda x,y:max(x,y))\n",
    "b.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46a25d2ee2cf42d8b5967fa87ecb381310ee627e58dce25c5456af41f27dd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
